{"nbformat": 4, "nbformat_minor": 0, "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "colab": {"name": "Session 11.ipynb", "provenance": [], "collapsed_sections": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.1"}}, "cells": [{"cell_type": "code", "metadata": {"id": "iRjddwOiWzr5", "tags": []}, "source": "%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport math", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "wjyOyZDn6rk2", "tags": []}, "source": "## Exercise 11.1: PCA on MNIST\n\nIn the lectures the principal component analysis (PCA) was introduced as a\nmethod for dimensionality reduction and feature extraction, i.e., to condense\ndata by mapping it to a lower dimensional space of the most important features.\n\nLet\n\\begin{equation*}\n  \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^\\intercal \\\\ \\vdots \\\\\n    \\mathbf{x}_N^\\intercal \\end{bmatrix} \\in \\mathbb{R}^{N \\times D}\n\\end{equation*}\nbe a matrix of $N$ data samples $\\mathbf{x}_n \\in \\mathbb{R}^D$, which are\ncentered around zero.\nWe consider a PCA with $M < D$ components.\n\nTo project the data points $\\mathbf{x}_n$ to the $M$-dimensional space that is\ndefined by the $M$ principal components of $\\mathbf{X}$, the so-called principal\nsubspace of $\\mathbf{X}$, we can use the singular value decomposition of\n$\\mathbf{X}$. Let $\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\intercal$ be the\nsingular value decomposition of the data matrix $\\mathbf{X}$ with the singular\nvalues sorted in descending order.\n\nThen the projection $\\mathbf{z}_n$ of\ndata point $\\mathbf{x}_n$ to the principal subspace of $\\mathbf{X}$ is given by\n\\begin{equation}\n  \\mathbf{z}_n^\\intercal = \\mathbf{x}_n^\\intercal \\begin{bmatrix} \\mathbf{v}_1 & \\cdots & \\mathbf{v}_M \\end{bmatrix},\n\\end{equation}\nwhere $\\mathbf{v}_i$ is the $i$th column of matrix $\\mathbf{V}$. The vector\n$\\mathbf{z}_n$ can be seen as an encoding of the data point\n$\\mathbf{x}_n$ in a lower dimensional space that is constructed by the directions\nfor which the data shows the largest variations.\n\n- **NOTE:** The\n  singular value decomposition of a matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$\n  is defined as a factorization of the form $\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma}\n  \\mathbf{V}^\\intercal$ where $\\mathbf{U} \\in \\mathbb{R}^{N \\times N}$ and\n  $\\mathbf{V} \\in \\mathbb{R}^{D \\times D}$ are orthogonal matrices and\n  $\\mathbf{\\Sigma} \\in \\mathbb{R}^{N \\times D}$ is a rectangular diagonal matrix with\n  non-negative numbers on the diagonal. The diagonal entries of $\\mathbf{\\Sigma}$ are\n  the so-called singular values of $\\mathbf{X}$. A common convention is to sort\n  the singular values in descending order, in which case the diagonal matrix $\\mathbf{\\Sigma}$ is uniquely determined by $\\mathbf{X}$.\n\nIn this exercise, we perform and analyse PCA of the MNIST image data set with $k = 2$ principal components."}, {"cell_type": "code", "metadata": {"tags": [], "id": "3_ghoR-Bxh7c"}, "source": "# We use torch and torchvision for automatically downloading the MNIST dataset\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n# download MNIST datasets and flatten images to vectors of length 784\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Lambda(lambda x: x.view(-1))])\ntrainset = torchvision.datasets.MNIST(root='./data', train=True,\n                                      download=True, transform=transform)\ntestset = torchvision.datasets.MNIST(root='./data', train=False,\n                                     download=True, transform=transform)\n\n# extract a complete PyTorch dataset\ndef extract(dataset):\n    datasize = len(dataset)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=datasize, shuffle=False)\n    return next(iter(dataloader))\n\n# extract all training and test images and labels into PyTorch tensors\ntrain_images, train_labels = extract(trainset)\ntest_images, test_labels = extract(testset)\n\n# It is possible to do the exercise both using torch or numpy.\n# In the second case, you can just convert the matrices to numpy using:\n# train_images = train_images.numpy()\n# train_labels = train_labels.numpy()\n# test_images = test_images.numpy()\n# test_labels = test_labels.numpy()", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "SMqxXreK5ghh", "tags": []}, "source": "\nLet $\\mathbf{X} \\in [0,1]^{N \\times 784}$ be a matrix of the MNIST training data set with $N = 60000$, where each row represents a training image. Moreover, let $\\mathbf{X} - \\overline{\\mathbf{x}}^\\intercal = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\intercal$ be the singular value decomposition of the MNIST training data after them mean has been removed, where $\\overline{\\mathbf{x}} = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{x}_i$.\n\nWe compute the singular value decomposition $\\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^\\intercal$ with the function [`torch.svd`](https://pytorch.org/docs/stable/torch.html#torch.svd)."}, {"cell_type": "code", "metadata": {"id": "wkfNjs3XbMyC", "tags": []}, "source": "# center training images\ntrain_mean = train_images.mean(axis=0)\ntrain_images_centered = train_images - train_mean\n\nU, S, V = torch.svd(train_images_centered)", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "g9Jt2du7O4z1", "tags": []}, "source": "### Item (a)\n\nLet `train_encoding` $\\in \\mathbb{R}^{60000 \\times 2}$ and `test_encoding` $\\in \\mathbb{R}^{10000 \\times 2}$ be the two-dimensional principal subspace of the MNIST training images. Compute the two encodings of the images in the MNIST training and the test data set.  Do it with the help of $\\mathbf{U}$, $\\boldsymbol{\\Sigma}$, and $\\mathbf{V}$.\n\n*Hints*: Rememeber that the presence of the center $\\overline{\\textbf{x}}$ needs to be accounted for.\n"}, {"cell_type": "code", "metadata": {"id": "XnM5TzfNxh7f", "tags": []}, "source": "", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "QqA3hX3ybMyZ", "tags": []}, "source": "Generate 2D scatter plots showing of the datapoints you computed in the last item. The function `plot_encoding` bellow can be used to generate a plot of the latent space."}, {"cell_type": "code", "metadata": {"id": "XnA0MGjBbMyc", "tags": []}, "source": "def plot_encoding(train_data, test_data):\n    # create two plots side by side\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n\n    # plot data\n    titles = [\"training data\", \"test data\"]\n    for (data, title, ax) in zip([train_data, test_data], titles, axes.flat):\n        encodings, labels = data\n        scatter = ax.scatter(encodings[:, 0], encodings[:, 1],\n                             c=labels, cmap=plt.cm.tab10, vmin=-0.5,\n                             vmax=9.5, alpha=0.7)\n        ax.set_xlabel(\"$z_1$\")\n        ax.set_ylabel(\"$z_2$\")\n        ax.set_title(title)\n\n    # add colorbar\n    cb = fig.colorbar(scatter, ticks=np.arange(0, 10), ax=axes.ravel().tolist())\n    cb.ax.set_title(\"digit\")\n    \n    return fig", "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 406}, "tags": [], "id": "CEC8YzTmxh7h", "outputId": "d0bc5690-6f2b-4140-d1d9-cbf1ebcec0ee"}, "source": "plot_encoding((train_encoding, train_labels), (test_encoding, test_labels))\nplt.show()", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "QjsGYI6pbMyt", "tags": []}, "source": "### Item (b)\n\nWith the help of the matrices $\\mathbf{U}$, $\\boldsymbol{\\Sigma}$, and $\\mathbf{V}$ computed in the last item. Map the `test_encoding` from the enconding space back to the space of image obtaining: `test_reconstruction` $\\in \\mathbb{R}^{10000 \\times 784}$ which will consist of the images in the MNIST test data set obtained considering only the two principal components."}, {"cell_type": "code", "metadata": {"id": "eNv8RcWWxh7k", "tags": []}, "source": "", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "QjtEPRQnbMyv", "tags": []}, "source": "Plot some test images and their reconstructed counterparts. You can use the function `plot_reconstruction` bellow do the plotting. Which digits can be reconstructed and decoded quite well, and which ones seem to be more challenging?"}, {"cell_type": "code", "metadata": {"id": "yaF1Pr4Sxh7k", "tags": []}, "source": "", "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 594}, "id": "pVhQvVPmbMyy", "tags": [], "outputId": "71c7cd7f-54d0-4d6e-9a0e-e52ce73ba5cf"}, "source": "plot_reconstruction(test_images, test_reconstruction, test_labels)\nplt.show()", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "bNR9-T54xh7l", "tags": []}, "source": "### Item (c)\n\nThe comparison of the original images and their reconstructions provides us with some intuition for how much information is lost by the compression of the images to the two-dimensional latent space. As a less subjective measure we calculate the average squared reconstruction error. An advantage of an objective measure such as the average squared reconstruction error is that it enables us to compare the PCA with other models for dimensionality reduction.\n\\begin{equation*}\n\\mathrm{sqerr} := \\frac{1}{10000} \\sum_{i=1}^{10000} \\|\\mathbf{x}_i - \\tilde{\\mathbf{x}}_i\\|^2_2\n\\end{equation*}\nof the images $\\mathbf{x}_i \\in {[0,1]}^{784}$ and their reconstruction $\\tilde{\\mathbf{x}}_i \\in \\mathbb{R}^{784}$ ($i = 1,\\ldots, 10000$) in the MNIST test data set. \n\nWhat average squared reconstruction error do you get with PCA?"}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Ho3lBFIyaKg6", "tags": [], "outputId": "f6d2d4ef-d839-4836-dd26-efc035e02ef9"}, "source": "sqerr = (test_images - test_reconstruction).pow(2).sum(dim=1).mean()\nprint(f\"Average squared reconstruction error: {sqerr}\")", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "rqr1928yo9M0", "tags": []}, "source": "### Item (d)\n\nFor each digit from 1 to 9, compute the mean of the data training points in the encoding space."}, {"cell_type": "code", "metadata": {"tags": [], "id": "z-lL95Elxh7i"}, "source": "def mean_encodings(encodings, labels):\n    # compute mean encodings\n    mean_encodings = []\n    for i in range(10):\n        mean_encoding = torch.mean(encodings[labels == i, :], dim=0)\n        mean_encodings.append(mean_encoding)\n    mean_encodings = torch.stack(mean_encodings, dim=0)\n\n    return mean_encodings\n# compute mean encoding\ntrain_mean_encodings = mean_encodings(train_encoding, train_labels)", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "WZawT13Jxh7j", "tags": []}, "source": "Generate the images corresponding to each of the means obtained in the encode space. Use the SVD matrix to convert the encoding space back to the image space. Use the the function `plot_images` bellow to generate a plot of that image."}, {"cell_type": "code", "metadata": {"tags": [], "id": "MfZfs9rZxh7j"}, "source": "def plot_images(images, labels=None, nrow=5):\n    # compute number of columns of grid\n    num_samples = images.size(0)\n    num_cols = min(nrow, num_samples)\n    num_rows = int(math.ceil(float(num_samples) / nrow))\n\n    fig = plt.figure(figsize=(2 * num_cols, 2 * num_rows))\n    for i in range(num_samples):\n        # extract image and labels if provided\n        image = images[i]\n        label = None if labels is None else labels[i]\n\n        # configure subplot\n        plt.subplot(num_rows, num_cols, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        if label is not None:\n            plt.title(f\"Label: {label.item()}\", fontweight='bold')\n\n        # plot image\n        plt.imshow(image.view(28, 28).cpu().numpy(),\n                   vmin=0.0, vmax=1.0, cmap='gray_r')\n        \n    return fig", "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 265}, "tags": [], "id": "wXhNi4p9xh7j", "outputId": "9cb7928a-e79e-4f4a-a9a3-fa43ddf1839f"}, "source": "# compute mean images\ntrain_mean_images = train_mean + train_mean_encodings.mm(V[:, :2].t())\n\nplot_images(train_mean_images, torch.arange(10))\nplt.show()", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "kXOdBYQm1R29", "tags": []}, "source": "## Exercise 11.2 - Derivations for probabilistic PCA\n\n\nIn constrast to (regular) PCA, the so-called probabilistic PCA (PPCA) allows a probabilistic\ninterpretation of the principal components. The probabilistic formulation\nof PCA also allows us to extend the method and alter its underlying assumptions\nquite easily, as we will learn during the course of this laboratory.\n\n\nAs before, let $\\mathbf{x} \\in \\mathbb{R}^D$ represent a data sample that we\nwant to decode from a lower dimensional representation\n$\\mathbf{z} \\in \\mathbb{R}^M$ with $M < D$. The PPCA model assumes that\n$\\mathbf{z}$ is standard normally distributed and $\\mathbf{x}$\ncan be decoded by a noisy linear transformation of $\\mathbf{z}$.\nMathematically, the model is given by\n\\begin{align*}\n  p(\\mathbf{x} \\,|\\, \\mathbf{z}) &= \\mathcal{N}\\left(\\mathbf{x}; \\mathbf{W}\\mathbf{z} + \\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_D\\right), \\\\\n  p(\\mathbf{z}) &= \\mathcal{N}(\\mathbf{z}; \\boldsymbol{0}, \\mathbf{I}_M),\n\\end{align*}\nwith parameters $\\mathbf{W} \\in \\mathbb{R}^{D \\times M}$,\n$\\boldsymbol{\\mu} \\in \\mathbb{R}^D$, and $\\sigma^2 > 0$.\n[Michael E. Tipping and Christopher M. Bishop show in \"Probabilistic Principal Component Analysis\"](https://www.jstor.org/stable/2680726)\nthat for $\\sigma^2 \\to 0$\nthe model recovers the standard PCA (but the components of $\\mathbf{z}$ might\nbe permuted).\n\nWe assume that the data $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$ is identically\nand independently distributed according to the PPCA model. In a maximum\nlikelihood setting, one determines the parameters $\\mathbf{W}$, $\\boldsymbol{\\mu}$,\nand $\\sigma^2$ that maximize the likelihood\n\\begin{equation*}\n  p(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N ; \\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2)\n  = \\prod_{n=1}^N p(\\mathbf{x}_n; \\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2),\n\\end{equation*}\nor equivalently the log-likelihood\n\\begin{equation*}\n  \\log p(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N; \\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2)\n  = \\sum_{n=1}^N \\log p(\\mathbf{x}_n; \\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2).\n\\end{equation*}"}, {"cell_type": "markdown", "metadata": {"id": "xQffYFcTy3jI", "tags": []}, "source": "### Item (a) \n\n*(Pen and paper exercise)*\n\nShow that for the model of the probabilistic PCA\n  \\begin{equation*}\n    p(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x}; \\boldsymbol{\\mu}, \\mathbf{C}),\n  \\end{equation*}\n  where $\\mathbf{C} = \\mathbf{W}\\mathbf{W}^\\intercal + \\sigma^2 \\mathbf{I}_D$.\n\n"}, {"cell_type": "markdown", "metadata": {"id": "3UCseyss1Vsu", "tags": []}, "source": "### Item (b)\n*(Pen and paper exercise)*\n\nShow that the distribution of the latent variable $\\mathbf{z}$ conditioned on\n  $\\mathbf{x}$ is Gaussian as well and given by\n  \\begin{equation*}\n    p(\\mathbf{z} \\,|\\, \\mathbf{x}) = \\mathcal{N}\\left(\\mathbf{z}; \\mathbf{M}^{-1} \\mathbf{W}^\\intercal (\\mathbf{x} - \\boldsymbol{\\mu}), \\sigma^2 \\mathbf{M}^{-1} \\right),\n  \\end{equation*}\n  where $\\mathbf{M} = \\mathbf{W}^\\intercal \\mathbf{W} + \\sigma^2 \\mathbf{I}_M$.\n\n\n"}, {"cell_type": "markdown", "metadata": {"id": "GR9ls8C-s_dd", "tags": []}, "source": "## Exercise 11.3 - Gaussian Mixture Models\n\n\nConsider the dataset generated bellow. It was generated using a gaussian mixture. That is the dataset is generated as follows:\n\n- We sample a variable `pp` from a bernouly distribution with probability `pi = 0.7`.\n- When the result is equal to one (`pp == 1`) we sample from the normal:\n$$\\mathcal{N}\\left(\\mu_1 = \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix}, \\Sigma_1 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}\\right)$$\n\n- When the result is equal to one (`pp == 0`) we sample from the normal:\n$$\\mathcal{N}\\left(\\mu_2 = \\begin{bmatrix} 5 \\\\ 7\\end{bmatrix}, \\Sigma_2 = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\right)$$\n\n\nIn this exercise, you will use an expectation maximization algorithm to estimate the parameters of gaussian mixture model to fit this dataset. Using what you implemented, you will apply this model to the two dimensional encondings from the exercise 11.1."}, {"cell_type": "code", "metadata": {"id": "YHHYEKlcs_TX", "colab": {"base_uri": "https://localhost:8080/", "height": 265}, "outputId": "12fc1ae0-dc34-4937-a668-996ff3167fb7", "tags": []}, "source": "", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "fluE4c4ZCRx1", "tags": []}, "source": "### Item (a)\n\nImplement a function `e_step`  that receives as input:\n-  the `gmm_data`: which is an `300 x 2` np array generated above;\n-  the `odds`: which is a list containing as elements the `pi` and `1 - pi`; and\n- the `vnormals`: a list  `[n1, n2]` of variable generated using: (`multivariate_normal`)[https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html]. See `vnormals` above.\n\nand outputs a vector of dimension `300 x 2 ` containing the probability of `pp = 1` for each of the points. This is equivalent, in each row:\n$$\\left(\\frac{\\pi_1\\mathcal{N}\\left(x; \\mu_1 , \\Sigma_1\\right)}{\\sum_i \\pi_i\\mathcal{N}\\left(x; \\mu_i , \\Sigma_i\\right)},  \\frac{\\pi_2\\mathcal{N}\\left(x; \\mu_2 , \\Sigma_2\\right)}{\\sum_i \\pi_i\\mathcal{N}\\left(x; \\mu_i , \\Sigma_i\\right)}\\right) $$\n\nThis computation corresponds to expectation step in the expectation maximization algorithm."}, {"cell_type": "code", "metadata": {"id": "pamMsGZ3Cdhx", "tags": []}, "source": "", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Ngfkjp6cBGzM", "tags": []}, "source": "Bellow the function uses the values produced by such function to computed to decide whether a point from the data is associated with each center."}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 265}, "id": "TlmAnsuLCfBR", "outputId": "5bbaf1ab-d4c6-4a23-fa90-7af12b451e2c", "tags": []}, "source": "", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "5a8a_4ny7IwG", "tags": []}, "source": "### Item (b)\n\nImplement a function `m_step`  that receives as input:\n-  the `gmm_data`: which is an `300 x 2` np array generated above;\n-  the `ws`: which is an `300 x 2` np array giving in the first row the probability of a point belonging to the first normal in the mixture. And, in the second row, the probability of a point belonging to the first normal in the mixture. I.e. similar to the output of the function `e_step` implemented above.\n\nIt outputs the parameters of the mixture:\n-  the `odds`: which is a list containing as elements the `pi` and `1 - pi`; and\n- the `vnormals`: a list  `[n1, n2]` of variable generated using: (`multivariate_normal`)[https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html]. See `vnormals` above.\n\nThis can be done using Eq. 10.16 from http://smlbook.org/."}, {"cell_type": "code", "metadata": {"id": "aaqRa9mDFpxP", "tags": []}, "source": "", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "rvqfsNgWCt7N", "tags": []}, "source": "You can use the values computed in the last few items as a sanity check:\n\n"}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "JbLA6_i_GDQP", "outputId": "5918740e-d92c-4e5c-c61c-b8deedb977a7", "tags": []}, "source": "", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "dq-x0rL8GPE5", "tags": []}, "source": "If everything went right, you should obtain values close to the ones used to generate the dataset:\n\n- `odds` $\\approx$ [0.7 0.3]\n- The first normal: $$\\mu_1 = \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix}, \\Sigma_1 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$$\n-  The first normal: $$\\mu_2 = \\begin{bmatrix} 5 \\\\ 7\\end{bmatrix}, \\Sigma_2 = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$$\n"}, {"cell_type": "markdown", "metadata": {"id": "5c31XGxiLb1D", "tags": []}, "source": "### Item (c)\nUse the code bellow to plot 3 iterations of the Expectation-Maximization algorithm in the dataset generated above."}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "id": "moydTocLvWbQ", "outputId": "351fa6bb-8556-4ec1-ed7d-12b0ec8a6cd4", "tags": []}, "source": "", "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "gIk96nSiPy1E", "tags": []}, "source": "### Item (d)\n\nAdapt the code above and apply the method to MNIST 2 dimensional encoding you generated in Exercise 11.1. You can choose how many centers you want to use and what initialization parameters."}, {"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "id": "mWSxgXA0P2wX", "outputId": "b6fe5546-3288-4a36-a121-7d1e8917c743", "tags": []}, "source": "", "execution_count": null, "outputs": []}]}