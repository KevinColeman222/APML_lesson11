{"cells": [{"cell_type": "code", "execution_count": null, "id": "618bcfd4", "metadata": {"tags": []}, "outputs": [], "source": "%matplotlib inline\n\nimport numpy as np\nfrom scipy import stats, optimize\nimport matplotlib.pyplot as plt"}, {"cell_type": "markdown", "id": "eb240ca9", "metadata": {"tags": []}, "source": "# Gaussian processes in numpy\n\n## Exercise 4.1: Sample from GP prior\n\nIn this exercises we will write the code needed to draw and plot samples of $f$ from a Gaussian process prior with squared exponential (or, equivalently, RBF) kernel\n\\begin{equation*}\nf \\sim \\mathcal{GP}(m, \\kappa), \\qquad m(x) = 0 \\text{ and } \\kappa(x, x') = \\sigma^2_f \\exp{\\Big(\u2212\\frac{1}{2l^2} {\\|x - x'\\|}^2\\Big)}.\n\\end{equation*}\nTo implement this, we choose a vector of $m$ test input points $\\mathbf{x}_*$. We will choose $\\mathbf{x}_*$ to contain sufficiently many points, such that it will *appear* as a continuous line on the screen. We then evaluate the $m \\times m$ covariance matrix $\\kappa(\\mathbf{x}_\u2217, \\mathbf{x}_\u2217)$ and thereafter generate samples from the multivariate normal distribution\n\\begin{equation*}\nf(\\mathbf{x}_\u2217) \\sim \\mathcal{N}\\big(m(\\mathbf{x}_\u2217), \\kappa(\\mathbf{x}_\u2217, \\mathbf{x}_\u2217)\\big).\n\\end{equation*}"}, {"cell_type": "markdown", "id": "0013d607", "metadata": {"tags": []}, "source": "### (a)\n\nUse `numpy.linspace` to construct a vector $\\mathbf{x}_*$ with $m = 101$ elements equally spaced from -5 to 5."}, {"cell_type": "code", "execution_count": null, "id": "b1051b9a", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "1ec0520b", "metadata": {"tags": []}, "source": "### (b)\n\nConstruct a mean vector $m(\\mathbf{x}_\u2217)$ with 101 elements all equal to zero and the $101 \\times 101$ covariance matrix $\\kappa(\\mathbf{x}_*, \\mathbf{x}_\u2217)$. The expression for $\\kappa(\\cdot, \\cdot)$ is given above. Let the hyperparameters be $\\ell^2 = 2$ and $\\sigma^2_f = 1$."}, {"cell_type": "code", "execution_count": null, "id": "87147e52", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "efda7b49", "metadata": {"tags": []}, "source": "### (c)\n\nUse `scipy.stats.multivariate_normal` (you might need to use the option `allow_singular=True`) to draw 25 samples $f^{(1)}(\\mathbf{x}_*), \\ldots, f^{(25)}(\\mathbf{x}_*)$ from the multivariate normal distribution $f(\\mathbf{x}_\u2217) \\sim \\mathcal{N}\\big(m(\\mathbf{x}_*), \\kappa(\\mathbf{x}_*, \\mathbf{x}_*)\\big)$."}, {"cell_type": "code", "execution_count": null, "id": "8860de51", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "7862dce0", "metadata": {"tags": []}, "source": "### (d)\n\nPlot the samples $f^{(1)}(\\mathbf{x}_\u2217), \\ldots, f^{(25)}(\\mathbf{x}_\u2217)$ versus the input vector $\\mathbf{x}_*$."}, {"cell_type": "code", "execution_count": null, "id": "af7c64e7", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "e59f6757", "metadata": {"tags": []}, "source": "### (e)\n\nTry another value of $\\ell$ and repeat (b)-(d). How do the two plots differ, and why?"}, {"cell_type": "code", "execution_count": null, "id": "d76df64f", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "3a807698", "metadata": {"tags": []}, "source": "## Exercise 4.2: GP posterior\n\nIn this exercise we will perform Gaussian process regression. That means, based on the $n$ observations $\\mathcal{D} = \\{x_i, f(x_i)\\}^n_{i=1}$ and the prior belief $f \\sim \\mathcal{GP}\\big(0, \\kappa(x, x')\\big)$, we want to find the posterior $p(f | \\mathcal{D})$. (In the previous problem, we were only concerned with the prior $p(f)$, not conditioned on having observed the data $\\mathcal{D}$.) We consider the same Gaussian process prior (same mean $m(x)$ and $\\kappa(x, x')$ and hyperparameters) as in the previous exercise."}, {"cell_type": "markdown", "id": "f0d751d1", "metadata": {"tags": []}, "source": "### (a)\n\nConstruct two vectors $\\mathbf{x} = [-4, -3, -1, 0, 2]^\\mathsf{T}$ and $\\mathbf{y} = [-2, 0, 1, 2, -1]^\\mathsf{T}$, which will be our training data (that is, $n = 5$)."}, {"cell_type": "code", "execution_count": null, "id": "a458b7ea", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "8631bc66", "metadata": {"tags": []}, "source": "### (b)\n\nKeep $\\mathbf{x}_\u2217$ as in the previous problem. In addition to the $m \\times m$ matrix $\\kappa(\\mathbf{x}_\u2217, \\mathbf{x}_\u2217)$, now also compute the $n \\times m$ matrix $\\kappa(\\mathbf{x}, \\mathbf{x}_\u2217)$ and the $n \\times n$ matrix $\\kappa(\\mathbf{x}, \\mathbf{x})$.\n\n*Hint:* You might find it useful to define a function that returns $\\kappa(x, x')$, taking $x$ and $x'$ as arguments."}, {"cell_type": "code", "execution_count": null, "id": "7b7b1b8a", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "87207f59", "metadata": {"tags": []}, "source": "### (c)\n\nUse the training data $\\mathbf{x}$, $\\mathbf{y}$ and the matrices constructed in (b) to compute the posterior mean $\\boldsymbol{\\mu}_{\\mathrm{posterior}}$ and the posterior\ncovariance $\\mathbf{K}_{\\mathrm{posterior}}$ for $\\mathbf{x}_\u2217$, by using the equations for conditional multivariate normal distribution."}, {"cell_type": "code", "execution_count": null, "id": "e2b12671", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "cce3cf6c", "metadata": {"tags": []}, "source": "### (d)\n\nIn a similar manner as in (c) and (d) in the previous problem, draw 25 samples from the multivariate distribution $f(\\mathbf{x}_\u2217) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathrm{posterior}}, \\mathbf{K}_{\\mathrm{posterior}})$ and plot these samples ($f^{(j)}\n(\\mathbf{x}_\u2217)$ vs. $\\mathbf{x}_\u2217$) together with the posterior mean\n($\\boldsymbol{\\mu}_{\\mathrm{posterior}}$ vs. $\\mathbf{x}_\u2217$) and the actual measurements ($\\mathbf{f}$ vs. $\\mathbf{x}$). How do the samples in this plot differ from the prior samples in the previous problem?"}, {"cell_type": "code", "execution_count": null, "id": "1f265ced", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "67d60cf1", "metadata": {"tags": []}, "source": "### (e)\n\nInstead of plotting samples, plot a credibility region. Here, a credibility region is based on the (marginal) posterior variance. The 68% credibility region, for example, is the area between $\\boldsymbol{\\mu}_{\\mathrm{posterior}} - \\sqrt{\\mathbf{K}^d_{\\mathrm{posterior}}}$ and\n$\\boldsymbol{\\mu}_{\\mathrm{posterior}} + \\sqrt{\\mathbf{K}^d_{\\mathrm{posterior}}}$, where $\\mathbf{K}^d_{\\mathrm{posterior}}$ is a vector with the diagonal elements of $\\mathbf{K}_{\\mathrm{posterior}}$. What is the connection\nbetween the credibility regions and the samples you drew previously?"}, {"cell_type": "code", "execution_count": null, "id": "6d781fb9", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "d5c4d698", "metadata": {"tags": []}, "source": "### (f)\n\nNow, consider the setting where the measurements are corrupted with noise, $y_i = f(\\mathbf{x}_i) + \\varepsilon$, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. Use $\\sigma^2 = 0.1$ and repeat (c)-(e) with this modification of the model. What is the difference in comparison to the previous plot? What is the interpretation?"}, {"cell_type": "code", "execution_count": null, "id": "bd40020b", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "39ff1cdb", "metadata": {"tags": []}, "source": "### (g)\n\nExplore what happens with another length scale $\\ell$."}, {"cell_type": "code", "execution_count": null, "id": "a1b44903", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "50548d5d", "metadata": {"tags": []}, "source": "## Exercise 4.3: Other covariance functions/kernels\n\nThe squared exponential kernel/covariance function gives samples which are smooth and infinitely continuously differentiable. Other kernels make other assumptions. Now try the previous problems using the exponential kernel instead,\n\\begin{equation*}\n\\kappa(x, x') = \\exp{\\Big(-\\frac{1}{l}\\|x - x'\\|\\Big)}.\n\\end{equation*}"}, {"cell_type": "code", "execution_count": null, "id": "da81bc1d", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "982d42e0", "metadata": {"tags": []}, "source": "## Exercise 4.4: Learning hyperparameters\n\n\nUntil now, we have made GP regression using predefined hyperparameters, such as the lengthscale $\\ell$ and noise variance $\\sigma^2$. In this exercise, we will estimate $\\ell$ and $\\sigma^2$ from the data by maximizing the marginal likelihood. The logarithm of the marginal likelihood for a Gaussian process observed with Gaussian noise is\n\\begin{equation*}\n\\log p(\\mathbf{y} | \\mathbf{x}; \\ell, \\sigma_f^2, \\sigma^2) = \\log \\mathcal{N}(\\mathbf{y} | 0, \\mathbf{K}_y) = -\\frac{1}{2} \\mathbf{y}^\\mathsf{T} \\mathbf{K}_y^{-1} \\mathbf{y} - \\frac{1}{2} \\log{|\\mathbf{K}_y|} - \\frac{n}{2}\\log{(2\\pi)}\n\\end{equation*}\nwhere $\\mathbf{K}_y = \\kappa(\\mathbf{x}, \\mathbf{x}) + \\sigma^2 \\mathbf{I}$."}, {"cell_type": "markdown", "id": "742ccd58", "metadata": {"tags": []}, "source": "### (a)\n\nWrite a function that takes $\\mathbf{x}$, $\\mathbf{y}$, $\\ell$, $\\sigma_f^2$, and $\\sigma^2$ as inputs and produces the logarithm of the marginal likelihood as output for the squared exponential covariance function."}, {"cell_type": "code", "execution_count": null, "id": "48d4a800", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "0d8e31dd", "metadata": {"tags": []}, "source": "### (b)\n\nConsider the same data as before. Use $\\sigma_f^2 = 1$ and $\\sigma^2 = 0$ and compute the logarithm of the marginal likelihood for values of $\\ell$ between 0.1 and 1 and plot it. What seems to be the maximal value of the marginal likelihood on this interval? Do GP regression based on this value of $\\ell$."}, {"cell_type": "code", "execution_count": null, "id": "abc7bc2a", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "65d67a3d", "metadata": {"tags": []}, "source": "## Exercise 4.5: Learning hyperparameters II\n\nIn this exercise we investigate a setting where the marginal likelihood has multiple local minima."}, {"cell_type": "markdown", "id": "f26ec3ba", "metadata": {"tags": []}, "source": "### (a)\n\nNow, consider the following data\n\\begin{equation*}\n\\mathbf{x} = [\u22125, \u22123, 0, 0.1, 1, 4.9, 5]^\\mathsf{T}, \\qquad\n\\mathbf{y} = [0, \u22120.5, 1, 0.7, 0, 1, 0.7]^\\mathsf{T}\n\\end{equation*}\nand compute the log marginal likelihood for both $\\ell$ and $\\sigma^2$. Use a logarithmic 2D-grid for values of $\\ell$ spanning from $10^{\u22121}$\nto $10^2$ and for $\\sigma^2$ spanning from $10^{\u22122}$ to $10^0$. Visualize the marginal likelihood on that grid with a contour plot."}, {"cell_type": "code", "execution_count": null, "id": "9663f991", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "4db9e1c5", "metadata": {"tags": []}, "source": "### (b)\n\nFind the hyperparameters $\\ell$ and $\\sigma^2$ that correspond to the maximal marginal likelihood. Perform GP regression on\nthe data using these hyperparameters."}, {"cell_type": "code", "execution_count": null, "id": "5a60bf5f", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "92a11275", "metadata": {"tags": []}, "source": "### (d)\n\nPerform GP regression for the hyperparameters that correspond to other possible local optima of the marginal likelihood. What differences do you see in your posterior?"}, {"cell_type": "code", "execution_count": null, "id": "f3ccecd1", "metadata": {"tags": []}, "outputs": [], "source": ""}], "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "kernelspec": {"display_name": "apml-exercises", "language": "python", "name": "apml-exercises"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.7"}}, "nbformat": 4, "nbformat_minor": 5}